# RAG-LLM
Conversational retrieval system implemented using the LangChain framework, using FAISS vectorstore. This project uses LLAMA-2 as LLM and instruct-xl as the Embedding Model.

<h2>Setup</h2>
1. Download the repository files<br>
2. Download llama2 model from https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/blob/main/llama-2-7b-chat.ggmlv3.q8_0.bin<br>
3. Set paths for LLM and vectordb in .env file<br>
4. In command line run :
  ```sh
  pip install -r requirements.txt
  ```
